{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TEORÍA - SESIÓN 4: Testing e Integración con IA Generativa\n",
        "\n",
        "**Duración:** 180 minutos\n",
        "\n",
        "**Objetivos:**\n",
        "- Implementar tests automatizados con pytest y TestClient\n",
        "- Mockear dependencias y autenticación en tests\n",
        "- Integrar APIs de IA generativa en FastAPI\n",
        "- Controlar tokens, costes y rate limiting\n",
        "- Manejar errores y timeouts en llamadas a IA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CONFIGURACIÓN DEL ENTORNO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instalación de dependencias\n",
        "!pip install fastapi==0.115.0 uvicorn[standard]==0.32.0 pytest==8.0.0 httpx==0.27.0 google-generativeai openai==1.12.0 python-dotenv==1.0.0 ipytest -q\n",
        "print(\" Dependencias instaladas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sobre las herramientas instaladas\n",
        "\n",
        "**ipytest:**\n",
        "- Permite ejecutar tests de pytest directamente en notebooks de Jupyter\n",
        "- Sin ipytest: Tendrías que crear archivos .py separados y ejecutar `pytest test_api.py` en terminal\n",
        "- Con ipytest: Puedes escribir y ejecutar tests en la misma celda del notebook\n",
        "- Útil para aprendizaje y prototipado rápido\n",
        "\n",
        "**google-generativeai:**\n",
        "- SDK oficial de Google para usar modelos Gemini\n",
        "- Alternativa a Google Gemini, con API gratuita para desarrollo\n",
        "- Usaremos Gemini 2.5 Flash para los ejemplos de integración con IA\n",
        "\n",
        "**Flujo normal en producción:**\n",
        "```bash\n",
        "# 1. Crear archivo de tests\n",
        "# test_api.py\n",
        "\n",
        "# 2. Ejecutar tests desde terminal\n",
        "pytest test_api.py -v\n",
        "\n",
        "# 3. Ver resultados en consola\n",
        "```\n",
        "\n",
        "En este notebook usamos ipytest para ver los resultados inmediatamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Servidor\n",
        "\n",
        "Una función para ejecutar el servidor sin tener que reiniciar el kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import uvicorn\n",
        "import threading\n",
        "import time\n",
        "import logging\n",
        "\n",
        "# Variable global para guardar la referencia al servidor activo\n",
        "# (Esto sobrevive entre ejecuciones de celdas)\n",
        "if \"active_server\" not in globals():\n",
        "    active_server = None\n",
        "\n",
        "\n",
        "def run_api(app_instance, port=8000):\n",
        "    global active_server\n",
        "\n",
        "    logging.getLogger(\"uvicorn\").setLevel(logging.CRITICAL)\n",
        "    logging.getLogger(\"uvicorn.error\").setLevel(logging.CRITICAL)\n",
        "    logging.getLogger(\"uvicorn.access\").setLevel(logging.CRITICAL)\n",
        "\n",
        "    # 1. SI YA HAY UN SERVIDOR, LO APAGAMOS\n",
        "    if active_server:\n",
        "        active_server.should_exit = True\n",
        "        active_server.force_exit = True\n",
        "\n",
        "        # Esperamos a que libere el puerto (max 3 segundos)\n",
        "        for _ in range(30):\n",
        "            if not active_server.started:\n",
        "                break\n",
        "            time.sleep(0.1)\n",
        "\n",
        "    # 2. CONFIGURAMOS EL NUEVO\n",
        "    config = uvicorn.Config(\n",
        "        app=app_instance, host=\"127.0.0.1\", port=port, log_level=\"warning\"\n",
        "    )\n",
        "    server = uvicorn.Server(config)\n",
        "\n",
        "    # Guardamos la referencia global\n",
        "    active_server = server\n",
        "\n",
        "    # 3. ARRANCAMOS EN UN HILO APARTE\n",
        "    thread = threading.Thread(target=server.run)\n",
        "    thread.daemon = True\n",
        "    thread.start()\n",
        "\n",
        "    time.sleep(1)\n",
        "    print(f\" Servidor iniciado en http://127.0.0.1:{port}\")\n",
        "    print(f\" Documentación: http://127.0.0.1:{port}/docs\")\n",
        "\n",
        "\n",
        "print(\" Definida correctamente\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# BLOQUE 1: TESTING\n",
        "\n",
        "## 1. INTRODUCCIÓN AL TESTING\n",
        "\n",
        "### ¿Por qué testear APIs?\n",
        "\n",
        "Los tests automatizados son esenciales en producción:\n",
        "\n",
        "1. **Detectar bugs antes de desplegar** → Evitar errores en producción\n",
        "2. **Documentación viva** → Los tests muestran cómo usar la API\n",
        "3. **Refactorización segura** → Cambiar código sin miedo a romper funcionalidad\n",
        "4. **CI/CD** → Integración continua con validación automática\n",
        "\n",
        "### TestClient de FastAPI\n",
        "\n",
        "FastAPI incluye `TestClient` basado en `httpx` para hacer requests sin levantar un servidor real.\n",
        "\n",
        "**Ventajas:**\n",
        "- No necesitas `uvicorn` corriendo\n",
        "- Tests rápidos (milisegundos)\n",
        "- Aislamiento total (cada test limpio)\n",
        "\n",
        "### Estructura básica con pytest\n",
        "\n",
        "```python\n",
        "\n",
        "# test_api.py\n",
        "from fastapi.testclient import TestClient\n",
        "\n",
        "def test_nombre_descriptivo():\n",
        "    # Arrange (preparar)\n",
        "    client = TestClient(app)\n",
        "    \n",
        "    # Act (actuar)\n",
        "    response = client.get(\"/ruta\")\n",
        "    \n",
        "    # Assert (verificar)\n",
        "    assert response.status_code == 200\n",
        "    \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ejemplo básico: API simple con test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# API simple\n",
        "from fastapi import FastAPI\n",
        "from fastapi import status\n",
        "from fastapi.testclient import TestClient\n",
        "app = FastAPI()\n",
        "\n",
        "@app.get(\"/\")\n",
        "def root():\n",
        "    return {\"mensaje\": \"API funcionando\"}\n",
        "\n",
        "@app.get(\"/saludo/{nombre}\")\n",
        "def saludar(nombre: str):\n",
        "    return {\"saludo\": f\"Hola {nombre}\"}\n",
        "\n",
        "# Test con TestClient\n",
        "client = TestClient(app)\n",
        "\n",
        "# Test 1: Endpoint raíz\n",
        "response = client.get(\"/\")\n",
        "assert response.status_code == 200\n",
        "assert response.json() == {\"mensaje\": \"API funcionando\"}\n",
        "print(\" Test 1 pasado: Endpoint raíz funciona\")\n",
        "\n",
        "# Test 2: Endpoint con parámetro\n",
        "response = client.get(\"/saludo/Ana\")\n",
        "assert response.status_code == 200\n",
        "assert response.json()[\"saludo\"] == \"Hola Ana\"\n",
        "print(\" Test 2 pasado: Saludo personalizado funciona\")\n",
        "\n",
        "# Iniciar servidor\n",
        "run_api(app, port=8500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. TESTS BÁSICOS\n",
        "\n",
        "### Status codes\n",
        "\n",
        "Los tests más comunes verifican los códigos HTTP correctos:\n",
        "\n",
        "- **200 OK** → GET exitoso\n",
        "- **201 Created** → POST crea recurso\n",
        "- **400 Bad Request** → Datos inválidos\n",
        "- **401 Unauthorized** → Sin autenticación\n",
        "- **404 Not Found** → Recurso no existe\n",
        "- **422 Unprocessable Entity** → Validación Pydantic falla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# API con validación Pydantic\n",
        "from fastapi import FastAPI\n",
        "from fastapi import status\n",
        "from fastapi.testclient import TestClient\n",
        "from pydantic import BaseModel\n",
        "app_validacion = FastAPI()\n",
        "\n",
        "class Item(BaseModel):\n",
        "    nombre: str\n",
        "    precio: float\n",
        "    cantidad: int = 1\n",
        "\n",
        "@app_validacion.post(\"/items\", status_code=201)\n",
        "def crear_item(item: Item):\n",
        "    return {\"item_creado\": item.model_dump()}\n",
        "\n",
        "# Tests de status codes\n",
        "client_val = TestClient(app_validacion)\n",
        "\n",
        "# Test 1: POST exitoso retorna 201\n",
        "response = client_val.post(\"/items\", json={\"nombre\": \"Laptop\", \"precio\": 999.99})\n",
        "assert response.status_code == 201\n",
        "print(\" Test: POST exitoso retorna 201\")\n",
        "\n",
        "# Test 2: POST con datos inválidos retorna 422\n",
        "response = client_val.post(\"/items\", json={\"nombre\": \"Laptop\"})  # Falta precio (obligatorio)\n",
        "assert response.status_code == 422\n",
        "print(\" Test: Validación Pydantic retorna 422\")\n",
        "\n",
        "# Test 3: Verificar estructura de error 422\n",
        "error_detail = response.json()[\"detail\"]\n",
        "assert len(error_detail) > 0  # Hay al menos un error\n",
        "assert error_detail[0][\"type\"] == \"missing\"  # Campo faltante\n",
        "print(\" Test: Error 422 tiene estructura correcta\")\n",
        "\n",
        "# Iniciar servidor\n",
        "run_api(app, port=8501)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validación de estructura de respuesta\n",
        "\n",
        "Además del status code, debemos verificar que el JSON retornado tenga la estructura esperada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test completo de estructura\n",
        "from fastapi import status\n",
        "response = client_val.post(\"/items\", json={\n",
        "    \"nombre\": \"Mouse\",\n",
        "    \"precio\": 25.50,\n",
        "    \"cantidad\": 3\n",
        "})\n",
        "\n",
        "assert response.status_code == 201\n",
        "data = response.json()\n",
        "\n",
        "# Verificar que existe la clave \"item_creado\"\n",
        "assert \"item_creado\" in data\n",
        "\n",
        "# Verificar estructura interna\n",
        "item = data[\"item_creado\"]\n",
        "assert item[\"nombre\"] == \"Mouse\"\n",
        "assert item[\"precio\"] == 25.50\n",
        "assert item[\"cantidad\"] == 3\n",
        "\n",
        "print(\" Test: Estructura de respuesta correcta\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MICRO-RETO 1: Test de endpoint con error\n",
        "\n",
        "Crea un test que verifique que un endpoint retorna 404 cuando se busca un recurso inexistente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# API con endpoint que puede retornar 404\n",
        "from fastapi import FastAPI\n",
        "from fastapi import HTTPException\n",
        "from fastapi import status\n",
        "from fastapi.testclient import TestClient\n",
        "app_404 = FastAPI()\n",
        "\n",
        "ITEMS_DB = {\n",
        "    1: {\"nombre\": \"Laptop\", \"precio\": 999.99},\n",
        "    2: {\"nombre\": \"Mouse\", \"precio\": 25.50}\n",
        "}\n",
        "\n",
        "@app_404.get(\"/items/{item_id}\")\n",
        "def obtener_item(item_id: int):\n",
        "    item = ITEMS_DB.get(item_id)\n",
        "    if not item:\n",
        "        raise HTTPException(status_code=404, detail=\"Item no encontrado\")\n",
        "    return item\n",
        "\n",
        "# TODO: Completa el test\n",
        "client_404 = TestClient(app_404)\n",
        "\n",
        "# Test 1: Item existente retorna 200\n",
        "# TODO: Completa aquí\n",
        "\n",
        "# Test 2: Item inexistente retorna 404\n",
        "# TODO: Completa aquí\n",
        "\n",
        "# Test 3: Verificar mensaje de error\n",
        "# TODO: Completa aquí\n",
        "\n",
        "# Iniciar servidor\n",
        "run_api(app, port=8502)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. FIXTURES Y MOCKING\n",
        "\n",
        "### ¿Qué son las fixtures en pytest?\n",
        "\n",
        "Las **fixtures** son funciones reutilizables que preparan el entorno para tests.\n",
        "\n",
        "**Ventajas:**\n",
        "- Evitar código duplicado\n",
        "- Setup/teardown automático\n",
        "- Tests más limpios y legibles\n",
        "\n",
        "**Ejemplo básico:**\n",
        "```python\n",
        "import pytest\n",
        "\n",
        "@pytest.fixture\n",
        "def datos_usuario():\n",
        "    return {\"nombre\": \"Ada\", \"edad\": 8}\n",
        "\n",
        "\n",
        "def test_edad(datos_usuario):\n",
        "    assert datos_usuario[\"edad\"] == 8\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ipytest\n",
        "ipytest.autoconfig()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%ipytest\n",
        "#  IMPORTANTE: Tienes que importar TODO lo que uses despues del %%ipytest\n",
        "from fastapi import FastAPI\n",
        "from fastapi.testclient import TestClient\n",
        "import pytest\n",
        "\n",
        "# --- DEFINICIÓN DE LA APP ---\n",
        "app = FastAPI()\n",
        "\n",
        "@app.get(\"/items\")\n",
        "def leer_items():\n",
        "    return [\"manzana\", \"banana\", \"cereza\"]\n",
        "\n",
        "# --- LA FIXTURE ---\n",
        "@pytest.fixture\n",
        "def cliente_test():\n",
        "    \"\"\"\n",
        "    Prepara el entorno y entrega el cliente.\n",
        "    \"\"\"\n",
        "    return TestClient(app)\n",
        "\n",
        "# --- TEST BUENO ---\n",
        "def test_leer_items(cliente_test):\n",
        "    # Pytest inyecta 'cliente_test' automáticamente\n",
        "    response = cliente_test.get(\"/items\")\n",
        "    \n",
        "    assert response.status_code == 200\n",
        "    assert len(response.json()) == 3\n",
        "    assert \"manzana\" in response.json()\n",
        "\n",
        "# --- TEST QUE FALLA ---\n",
        "def test_leer_items2(cliente_test):\n",
        "    response = cliente_test.get(\"/items\")\n",
        "    assert response.status_code == 200\n",
        "    # CAMBIO AQUÍ: Esperamos algo que no existe\n",
        "    assert \"piña\" in response.json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### app.dependency_overrides: Mockear dependencias\n",
        "\n",
        "En tests, a menudo queremos **mockear** (simular) dependencias como autenticación, bases de datos, etc.\n",
        "\n",
        "FastAPI tiene `app.dependency_overrides` para reemplazar dependencias en tests.\n",
        "\n",
        "**Caso de uso típico:** Mockear autenticación para no necesitar tokens reales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# API con autenticación\n",
        "from fastapi import Depends\n",
        "from fastapi import FastAPI\n",
        "from fastapi import HTTPException\n",
        "from fastapi import status\n",
        "from fastapi.testclient import TestClient\n",
        "from pydantic import BaseModel\n",
        "from typing import Annotated\n",
        "app_auth = FastAPI()\n",
        "\n",
        "class User(BaseModel):\n",
        "    username: str\n",
        "    email: str\n",
        "\n",
        "# Dependencia real (simula validar token)\n",
        "def get_current_user() -> User:\n",
        "    # En producción: validaría JWT, buscaría en BD, etc.\n",
        "    raise HTTPException(status_code=401, detail=\"No autenticado\")\n",
        "\n",
        "CurrentUser = Annotated[User, Depends(get_current_user)]\n",
        "\n",
        "@app_auth.get(\"/perfil\")\n",
        "def obtener_perfil(current_user: CurrentUser):\n",
        "    return current_user\n",
        "\n",
        "# SIN MOCK: Endpoint retorna 401\n",
        "client_auth = TestClient(app_auth)\n",
        "response = client_auth.get(\"/perfil\")\n",
        "assert response.status_code == 401\n",
        "print(\" Sin mock: Retorna 401 como esperado\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CON MOCK: Reemplazar dependencia\n",
        "from fastapi import status\n",
        "def mock_get_current_user() -> User:\n",
        "    \"\"\"Usuario mockeado para tests\"\"\"\n",
        "    return User(username=\"testuser\", email=\"test@example.com\")\n",
        "\n",
        "# Reemplazar dependencia SOLO en tests\n",
        "app_auth.dependency_overrides[get_current_user] = mock_get_current_user\n",
        "\n",
        "# Ahora el endpoint retorna 200 con usuario mockeado\n",
        "response = client_auth.get(\"/perfil\")\n",
        "assert response.status_code == 200\n",
        "assert response.json()[\"username\"] == \"testuser\"\n",
        "print(\" Con mock: Retorna 200 con usuario mockeado\")\n",
        "\n",
        "# IMPORTANTE: Limpiar después del test\n",
        "app_auth.dependency_overrides.clear()\n",
        "print(\" Override limpiado\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Patrón completo: Test con fixture + mock"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%ipytest\n",
        "from fastapi import Depends\n",
        "from fastapi import FastAPI\n",
        "from fastapi import HTTPException\n",
        "from fastapi import status\n",
        "from fastapi.testclient import TestClient\n",
        "from typing import Annotated\n",
        "import pytest\n",
        "\n",
        "# Aplicación de ejemplo\n",
        "app = FastAPI()\n",
        "\n",
        "# Dependencia real\n",
        "def obtener_usuario():\n",
        "    raise HTTPException(status_code=401, detail=\"No autenticado\")\n",
        "\n",
        "# Annotated reutilizable\n",
        "UsuarioActual = Annotated[dict, Depends(obtener_usuario)]\n",
        "\n",
        "@app.get(\"/protegido\")\n",
        "def ruta_protegida(usuario: UsuarioActual):\n",
        "    return {\"mensaje\": f\"Hola {usuario['nombre']}\"}\n",
        "\n",
        "# Fixture con mock\n",
        "@pytest.fixture\n",
        "def cliente_con_mock():\n",
        "    \"\"\"Fixture que retorna cliente con autenticación mockeada\"\"\"\n",
        "    \n",
        "    # Mock de la dependencia\n",
        "    def mock_usuario():\n",
        "        return {\"id\": 1, \"nombre\": \"usuario_prueba\"}\n",
        "    \n",
        "    # Reemplazar dependencia\n",
        "    app.dependency_overrides[obtener_usuario] = mock_usuario\n",
        "    \n",
        "    cliente = TestClient(app)\n",
        "    yield cliente\n",
        "    \n",
        "    # Limpieza\n",
        "    app.dependency_overrides.clear()\n",
        "\n",
        "# Test usando la fixture\n",
        "def test_protegido_con_mock(cliente_con_mock):\n",
        "    respuesta = cliente_con_mock.get(\"/protegido\")\n",
        "    assert respuesta.status_code == 200\n",
        "    assert \"usuario_prueba\" in respuesta.json()[\"mensaje\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  MICRO-RETO 2: Mockear base de datos\n",
        "\n",
        "Crea un test que mockee una función de \"buscar en BD\" para retornar datos fake."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from fastapi import Depends\n",
        "from fastapi import FastAPI\n",
        "app_db = FastAPI()\n",
        "\n",
        "# Dependencia que simula consulta a BD\n",
        "def get_db_connection():\n",
        "    # En producción: retornaría conexión real a PostgreSQL, etc.\n",
        "    raise Exception(\"BD no disponible en tests\")\n",
        "\n",
        "@app_db.get(\"/productos\")\n",
        "def listar_productos(db=Depends(get_db_connection)):\n",
        "    # En producción: db.query(...)\n",
        "    return db\n",
        "\n",
        "# TODO: Crea una función mock_db que retorne una lista fake de productos\n",
        "def mock_db():\n",
        "    # TODO: Retorna [{\"id\": 1, \"nombre\": \"Laptop\"}, {\"id\": 2, \"nombre\": \"Mouse\"}]\n",
        "    pass\n",
        "\n",
        "# TODO: Reemplaza la dependencia\n",
        "\n",
        "# TODO: Haz un test GET /productos y verifica que retorna la lista fake\n",
        "\n",
        "# TODO: Limpia el override\n",
        "\n",
        "# Iniciar servidor\n",
        "run_api(app, port=8507)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "#  BLOQUE 2: INTEGRACIÓN CON IA GENERATIVA\n",
        "\n",
        "## 1. INTEGRACIÓN CON APIS DE IA\n",
        "\n",
        "### Llamada a Google Gemini\n",
        "\n",
        "La librería `openai` (v1+) es compatible con múltiples proveedores:\n",
        "\n",
        "- **Google Gemini** (gemini-2.5-pro, Gemini 2.5 Flash)\n",
        "- **Anthropic** (Claude) → Compatible con Google Gemini SDK\n",
        "- **Otros** (con base_url personalizada)\n",
        "\n",
        "### Configuración de API keys\n",
        "\n",
        "** IMPORTANTE:** NUNCA hardcodear API keys en el código."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# 1. Configuración\n",
        "# Lo ideal es os.getenv(\"GEMINI_API_KEY\"), pero para el notebook:\n",
        "GOOGLE_API_KEY = \"AIzaSyBS7oO2Bw7GbVOBz1xOh5XHSMaSSXr6xkg\" \n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# 2. Definir el modelo\n",
        "model_name = 'models/gemini-2.5-flash'\n",
        "model = genai.GenerativeModel(model_name)\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Modelo de datos (Input)\n",
        "class PromptRequest(BaseModel):\n",
        "    prompt: str\n",
        "\n",
        "# 3. Endpoint\n",
        "@app.post(\"/generar-texto\")\n",
        "def generar_texto(request: PromptRequest):\n",
        "    try:\n",
        "        # Llamada asíncrona a Google\n",
        "        response = model.generate_content(request.prompt)\n",
        "        \n",
        "        # Extraer el texto limpio\n",
        "        return {\"respuesta\": response.text}\n",
        "        \n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "    \n",
        "\n",
        "#PRUEBA DE CONEXIÓN AL MODELO\n",
        "# 1. Creamos el cliente conectado a tu app\n",
        "cliente = TestClient(app)\n",
        "\n",
        "print(\"⏳ Enviando petición a Gemini (esto puede tardar unos segundos)...\")\n",
        "\n",
        "# 2. Hacemos la petición POST\n",
        "response = cliente.post(\n",
        "    \"/generar-texto\", \n",
        "    json={\"prompt\": \"Explica en 20 palabras qué es la IA generativa\"}\n",
        ")\n",
        "\n",
        "# 3. Verificamos y mostramos el resultado\n",
        "if response.status_code == 200:\n",
        "    data = response.json()\n",
        "    print(\"\\n RESPUESTA DE GEMINI:\\n\")\n",
        "    print(data[\"respuesta\"])  # Imprimimos solo el texto limpio\n",
        "else:\n",
        "    print(f\"\\n Error {response.status_code}:\")\n",
        "    print(response.json())\n",
        "\n",
        "\n",
        "# servidor\n",
        "run_api(app, port=8001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parámetros avanzados de generación\n",
        "\n",
        "Además del prompt básico, Gemini permite controlar el comportamiento de la generación:\n",
        "\n",
        "**generation_config:**\n",
        "```python\n",
        "generation_config = {\n",
        "    \"temperature\": 0.7,        # Creatividad (0-1): 0=determinista, 1=creativo\n",
        "    \"max_output_tokens\": 100,  # Límite de tokens generados\n",
        "    \"top_p\": 0.95,            # Nucleus sampling\n",
        "    \"top_k\": 40               # Top-k sampling\n",
        "}\n",
        "```\n",
        "\n",
        "**Parámetros clave:**\n",
        "- **temperature:** Controla aleatoriedad. Bajo (0.2) = respuestas precisas y predecibles. Alto (0.9) = respuestas creativas y variadas.\n",
        "- **max_output_tokens:** Límite máximo de tokens en la respuesta (equivalente a max_tokens en otros LLMs)\n",
        "- **top_p y top_k:** Controlan diversidad en la selección de tokens\n",
        "\n",
        "**System instructions:**\n",
        "```python\n",
        "model = genai.GenerativeModel(\n",
        "    \"gemini-2.5-flash\",\n",
        "    system_instruction=\"Eres un asistente experto en Python que responde de forma concisa\"\n",
        ")\n",
        "```\n",
        "\n",
        "Estas instrucciones guían el comportamiento general del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Ejemplo con parámetros de generación ===\n",
            "Respuesta: FastAPI es un framework web de Python moderno y de alto rendimiento diseñado para construir APIs RESTful de forma rápida y eficiente.\n",
            "\n",
            "A\n",
            "Tokens usados: 722\n",
            "Temperatura: 0.7\n",
            "\n",
            "Puedes experimentar con diferentes valores de temperatura (0.0-1.0)\n",
            "- Temperatura baja (0.2): Respuestas más precisas y deterministas\n",
            "- Temperatura alta (0.9): Respuestas más creativas y variadas\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "import asyncio\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from fastapi.testclient import TestClient\n",
        "\n",
        "# Configurar Gemini con system instruction\n",
        "genai.configure(api_key=\"AIzaSyBS7oO2Bw7GbVOBz1xOh5XHSMaSSXr6xkg\")\n",
        "\n",
        "model_avanzado = genai.GenerativeModel(\n",
        "    \"gemini-2.5-flash\",\n",
        "    system_instruction=\"Eres un asistente técnico que explica conceptos de programación de forma clara y concisa.\"\n",
        ")\n",
        "\n",
        "app_avanzada = FastAPI()\n",
        "\n",
        "class SolicitudAvanzada(BaseModel):\n",
        "    prompt: str\n",
        "    temperatura: float = 0.7\n",
        "    max_tokens: int = 150\n",
        "\n",
        "@app_avanzada.post(\"/completar-avanzado\")\n",
        "async def completar_con_parametros(solicitud: SolicitudAvanzada):\n",
        "    \"\"\"Endpoint con control de parámetros de generación\"\"\"\n",
        "    try:\n",
        "        # Configuración de generación\n",
        "        config_generacion = genai.types.GenerationConfig(\n",
        "            temperature=solicitud.temperatura,\n",
        "            max_output_tokens=solicitud.max_tokens,\n",
        "            top_p=0.95,\n",
        "            top_k=40\n",
        "        )\n",
        "        \n",
        "        # Llamada a Gemini con parámetros\n",
        "        respuesta = await model_avanzado.generate_content_async(\n",
        "            solicitud.prompt,\n",
        "            generation_config=config_generacion\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            \"respuesta\": respuesta.text,\n",
        "            \"tokens_usados\": respuesta.usage_metadata.total_token_count,\n",
        "            \"temperatura_usada\": solicitud.temperatura\n",
        "        }\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "# Prueba con temperatura media (equilibrio entre precisión y creatividad)\n",
        "cliente = TestClient(app_avanzada)\n",
        "\n",
        "print(\"=== Ejemplo con parámetros de generación ===\")\n",
        "respuesta = cliente.post(\"/completar-avanzado\", json={\n",
        "    \"prompt\": \"Explica qué es FastAPI en 2 frases\",\n",
        "    \"temperatura\": 0.7,\n",
        "    \"max_tokens\": 500\n",
        "})\n",
        "\n",
        "if respuesta.status_code == 200:\n",
        "    datos = respuesta.json()\n",
        "    print(f\"Respuesta: {datos['respuesta']}\")\n",
        "    print(f\"Tokens usados: {datos['tokens_usados']}\")\n",
        "    print(f\"Temperatura: {datos['temperatura_usada']}\")\n",
        "    print(\"\\nPuedes experimentar con diferentes valores de temperatura (0.0-1.0)\")\n",
        "    print(\"- Temperatura baja (0.2): Respuestas más precisas y deterministas\")\n",
        "    print(\"- Temperatura alta (0.9): Respuestas más creativas y variadas\")\n",
        "else:\n",
        "    print(f\"Error {respuesta.status_code}:\", respuesta.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  MICRO-RETO 3: Endpoint con prompt personalizado\n",
        "\n",
        "Crea un endpoint `/traducir` que reciba texto y lo \"traduzca\" usando un system prompt específico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'texto_original': 'Hello world, I am learning how to test FasAPI with AI integration', 'idioma': 'español', 'traduccion': 'Hola mundo, estoy aprendiendo a probar FastAPI con integración de IA.'}\n",
            " Servidor iniciado en http://127.0.0.1:8000\n",
            " Documentación: http://127.0.0.1:8000/docs\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "import asyncio\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "from fastapi.testclient import TestClient\n",
        "\n",
        "# Configurar Gemini\n",
        "genai.configure(api_key=\"AIzaSyBS7oO2Bw7GbVOBz1xOh5XHSMaSSXr6xkg\")\n",
        "model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
        "\n",
        "app_traductor = FastAPI()\n",
        "\n",
        "# TODO: Define TraduccionRequest con texto e idioma_destino\n",
        "class TraduccionRequest(BaseModel):\n",
        "    pass  # Completa aquí\n",
        "\n",
        "# TODO: Crea endpoint POST /traducir\n",
        "@app_traductor.post(\"/traducir\")\n",
        "async def traducir(request: TraduccionRequest):\n",
        "    # TODO: 1. Construye prompt\n",
        "    \n",
        "    # TODO: 2. Llama a Gemini\n",
        "    \n",
        "    # TODO: 3. Retorna traducción\n",
        "    pass\n",
        "\n",
        "# TODO: Test\n",
        "client = TestClient(app_traductor)\n",
        "response = client.post(\"/traducir\", json={\n",
        "    \"texto\": \"Hello world\",\n",
        "    \"idioma_destino\": \"español\"\n",
        "})\n",
        "print(response.json())\n",
        "\n",
        "\n",
        "# Levantar servidor\n",
        "run_api(app_traductor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. CONTROL DE TOKENS Y COSTES\n",
        "\n",
        "### Token limits\n",
        "\n",
        "Los LLMs tienen límites de tokens:\n",
        "\n",
        "- **gemini-2.5-flash**: 4,096 tokens (input + output)\n",
        "- **gemini-2.5-pro**: 8,192 tokens\n",
        "- **gemini-2.5-pro-32k**: 32,768 tokens\n",
        "\n",
        "**1 token ≈ 0.75 palabras en inglés**\n",
        "\n",
        "### Estimación de costes\n",
        "\n",
        "**Precios aproximados (gemini-2.5-flash):**\n",
        "- Input: $0.50 / 1M tokens\n",
        "- Output: $2.50 / 1M tokens\n",
        "\n",
        "**Ejemplo:**\n",
        "- Request con 100 tokens input + 200 tokens output\n",
        "- Coste: (100 × $0.50 + 200 × $2.50) / 1,000,000 = $0.00035"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coste: $0.000067 USD\n",
            "Con 1000 requests: $0.07 USD\n"
          ]
        }
      ],
      "source": [
        "# Cálculo de costes con Gemini\n",
        "PRECIOS_GEMINI = {\n",
        "    \"gemini-2.5-flash\": {\"input\": 0.075 / 1_000_000, \"output\": 0.30 / 1_000_000},\n",
        "    \"gemini-2.5-pro\": {\"input\": 1.25 / 1_000_000, \"output\": 5.00 / 1_000_000},\n",
        "}\n",
        "\n",
        "def calcular_coste(modelo: str, prompt_tokens: int, output_tokens: int) -> float:\n",
        "    \"\"\"Calcula coste en USD para modelos Gemini\"\"\"\n",
        "    precios = PRECIOS_GEMINI.get(modelo, PRECIOS_GEMINI[\"gemini-2.5-flash\"])\n",
        "    \n",
        "    coste_input = prompt_tokens * precios[\"input\"]\n",
        "    coste_output = output_tokens * precios[\"output\"]\n",
        "    \n",
        "    return coste_input + coste_output\n",
        "\n",
        "# Ejemplo\n",
        "coste = calcular_coste(\"gemini-2.5-flash\", prompt_tokens=100, output_tokens=200)\n",
        "print(f\"Coste: ${coste:.6f} USD\")\n",
        "print(f\"Con 1000 requests: ${coste * 1000:.2f} USD\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Rate limiting básico\n",
        "\n",
        "Para controlar costes y evitar abusos, implementamos rate limiting (limitación de velocidad).\n",
        "\n",
        "**¿Qué es rate limiting?**\n",
        "- Limita cuántas peticiones puede hacer un usuario en un periodo de tiempo\n",
        "- Ejemplo: Máximo 10 requests por minuto por usuario\n",
        "- Si se excede → retornar error 429 (Too Many Requests)\n",
        "\n",
        "**Parámetros clave:**\n",
        "- `user_id`: Identificador único del usuario (puede ser IP, token, etc.)\n",
        "- `max_requests`: Número máximo de peticiones permitidas (ej: 10)\n",
        "- `window_minutes`: Ventana temporal en minutos (ej: 1 minuto)\n",
        "\n",
        "**Patrón de ventana deslizante (sliding window):**\n",
        "- Se guarda el timestamp de cada request\n",
        "- Solo se cuentan los requests dentro de la ventana temporal actual\n",
        "- Los requests antiguos (fuera de la ventana) se descartan automáticamente\n",
        "\n",
        "**En producción:**\n",
        "- Usar Redis o Memcached (no defaultdict en memoria)\n",
        "- Implementar cleanup periódico de datos antiguos\n",
        "- Considerar rate limiting por endpoint, no solo global"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Simulación de rate limiting (límite: 10 requests/minuto)\n",
            "\n",
            "Request  1: Permitido\n",
            "Request  2: Permitido\n",
            "Request  3: Permitido\n",
            "Request  4: Permitido\n",
            "Request  5: Permitido\n",
            "Request  6: Permitido\n",
            "Request  7: Permitido\n",
            "Request  8: Permitido\n",
            "Request  9: Permitido\n",
            "Request 10: Permitido\n",
            "Request 11: BLOQUEADO - Rate limit excedido\n",
            "Request 12: BLOQUEADO - Rate limit excedido\n",
            "\n",
            "Total requests registrados para usuario_123: 10\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Almacén simple en memoria (en producción: usar Redis)\n",
        "# defaultdict crea automáticamente una lista vacía si la clave no existe\n",
        "rate_limit_store = defaultdict(list)\n",
        "\n",
        "def verificar_rate_limit(user_id: str, max_requests: int = 10, window_minutes: int = 1) -> bool:\n",
        "    \"\"\"\n",
        "    Verifica si el usuario excedió el límite de requests\n",
        "    \n",
        "    Args:\n",
        "        user_id: Identificador único del usuario\n",
        "        max_requests: Máximo de peticiones permitidas\n",
        "        window_minutes: Ventana temporal en minutos\n",
        "    \n",
        "    Returns:\n",
        "        True si la petición está permitida, False si excede el límite\n",
        "    \"\"\"\n",
        "    ahora = datetime.now()\n",
        "    \n",
        "    # Calcular el inicio de la ventana temporal\n",
        "    # Ejemplo: si son las 10:05 y window=1, inicio es 10:04\n",
        "    inicio_ventana = ahora - timedelta(minutes=window_minutes)\n",
        "    \n",
        "    # Filtrar solo los requests que están dentro de la ventana temporal\n",
        "    # Descartamos los requests antiguos (fuera de la ventana)\n",
        "    requests_recientes = [\n",
        "        timestamp for timestamp in rate_limit_store[user_id]\n",
        "        if timestamp > inicio_ventana\n",
        "    ]\n",
        "    \n",
        "    # Actualizar el store solo con requests recientes\n",
        "    rate_limit_store[user_id] = requests_recientes\n",
        "    \n",
        "    # Verificar si se excedió el límite\n",
        "    if len(requests_recientes) >= max_requests:\n",
        "        return False  # Rate limit excedido\n",
        "    \n",
        "    # Registrar este nuevo request\n",
        "    rate_limit_store[user_id].append(ahora)\n",
        "    return True  # Request permitido\n",
        "\n",
        "# Simulación: Usuario hace 12 requests (límite es 10)\n",
        "print(\"Simulación de rate limiting (límite: 10 requests/minuto)\\n\")\n",
        "\n",
        "for i in range(12):\n",
        "    permitido = verificar_rate_limit(\"usuario_123\", max_requests=10, window_minutes=1)\n",
        "    \n",
        "    if permitido:\n",
        "        print(f\"Request {i+1:2d}: Permitido\")\n",
        "    else:\n",
        "        print(f\"Request {i+1:2d}: BLOQUEADO - Rate limit excedido\")\n",
        "\n",
        "# Estado final del store\n",
        "print(f\"\\nTotal requests registrados para usuario_123: {len(rate_limit_store['usuario_123'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Integración en endpoint con rate limit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Request 1: OK - Restantes: 2\n",
            "Request 2: OK - Restantes: 1\n",
            "Request 3: OK - Restantes: 0\n",
            "Request 4: BLOQUEADO - Rate limit excedido. Máximo 3 requests por minuto\n"
          ]
        }
      ],
      "source": [
        "from fastapi import FastAPI, HTTPException, Header\n",
        "from pydantic import BaseModel\n",
        "from fastapi.testclient import TestClient\n",
        "from typing import Annotated\n",
        "from collections import defaultdict\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Rate limiting store\n",
        "rate_limit_store = defaultdict(list)\n",
        "\n",
        "def verificar_rate_limit(user_id: str, max_requests: int = 3) -> bool:\n",
        "    \"\"\"Verifica rate limit (3 requests por minuto)\"\"\"\n",
        "    ahora = datetime.now()\n",
        "    inicio_ventana = ahora - timedelta(minutes=1)\n",
        "    \n",
        "    requests_recientes = [\n",
        "        t for t in rate_limit_store[user_id] if t > inicio_ventana\n",
        "    ]\n",
        "    rate_limit_store[user_id] = requests_recientes\n",
        "    \n",
        "    if len(requests_recientes) >= max_requests:\n",
        "        return False\n",
        "    \n",
        "    rate_limit_store[user_id].append(ahora)\n",
        "    return True\n",
        "\n",
        "# Mock de Gemini para esta demostración (evita problemas de event loop)\n",
        "class MockGeminiResponse:\n",
        "    def __init__(self, texto):\n",
        "        self.text = texto\n",
        "        self.usage_metadata = type('obj', (object,), {'total_token_count': 50})\n",
        "\n",
        "async def mock_gemini(prompt: str):\n",
        "    \"\"\"Mock que simula llamada a Gemini (sin event loop issues)\"\"\"\n",
        "    return MockGeminiResponse(f\"Respuesta simulada a: {prompt[:20]}...\")\n",
        "\n",
        "# Aplicación\n",
        "app_con_rate_limit = FastAPI()\n",
        "\n",
        "class SolicitudIA(BaseModel):\n",
        "    prompt: str\n",
        "\n",
        "@app_con_rate_limit.post(\"/generar\")\n",
        "async def generar_con_rate_limit(\n",
        "    solicitud: SolicitudIA,\n",
        "    user_id: Annotated[str, Header()] = \"anonimo\"\n",
        "):\n",
        "    \"\"\"Endpoint con rate limiting integrado\"\"\"\n",
        "    \n",
        "    # Verificar rate limit ANTES de llamar a IA\n",
        "    if not verificar_rate_limit(user_id, max_requests=3):\n",
        "        raise HTTPException(\n",
        "            status_code=429,\n",
        "            detail=\"Rate limit excedido. Máximo 3 requests por minuto\"\n",
        "        )\n",
        "    \n",
        "    # Llamar a IA (mock para evitar problemas de event loop)\n",
        "    respuesta = await mock_gemini(solicitud.prompt)\n",
        "    \n",
        "    return {\n",
        "        \"respuesta\": respuesta.text,\n",
        "        \"requests_restantes\": 3 - len(rate_limit_store[user_id])\n",
        "    }\n",
        "\n",
        "# Demostración: 4 requests (límite es 3)\n",
        "cliente = TestClient(app_con_rate_limit)\n",
        "\n",
        "for i in range(1, 5):\n",
        "    response = cliente.post(\n",
        "        \"/generar\",\n",
        "        json={\"prompt\": \"Hola\"},\n",
        "        headers={\"user-id\": \"usuario_test\"}\n",
        "    )\n",
        "    \n",
        "    if response.status_code == 200:\n",
        "        datos = response.json()\n",
        "        print(f\"Request {i}: OK - Restantes: {datos['requests_restantes']}\")\n",
        "    else:\n",
        "        print(f\"Request {i}: BLOQUEADO - {response.json()['detail']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  MICRO-RETO 4: Control de max_tokens\n",
        "\n",
        "Modifica el endpoint para rechazar requests con `max_tokens > 500` (código 400)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "import asyncio\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from fastapi.testclient import TestClient\n",
        "\n",
        "# Configurar Gemini\n",
        "genai.configure(api_key=\"AIzaSyBS7oO2Bw7GbVOBz1xOh5XHSMaSSXr6xkg\")\n",
        "model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
        "\n",
        "app_validado = FastAPI()\n",
        "\n",
        "class CompletionRequest(BaseModel):\n",
        "    prompt: str\n",
        "    max_tokens: int = 100\n",
        "\n",
        "@app_validado.post(\"/completar\")\n",
        "async def completar_con_validacion(request: CompletionRequest):\n",
        "    # TODO: 1. Valida que max_tokens <= 500\n",
        "    \n",
        "    # TODO: 2. Llama a Gemini\n",
        "    \n",
        "    # TODO: 3. Retorna datos con tokens usados\n",
        "    pass\n",
        "\n",
        "# TODO: Test 1 - Debe fallar (400)\n",
        "client = TestClient(app_validado)\n",
        "response = client.post(\"/completar\", json={\"prompt\": \"test\", \"max_tokens\": 600})\n",
        "assert response.status_code == 400\n",
        "\n",
        "# TODO: Test 2 - Debe funcionar (200)\n",
        "response = client.post(\"/completar\", json={\"prompt\": \"Explica FastAPI\", \"max_tokens\": 100})\n",
        "assert response.status_code == 200\n",
        "print(response.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. MANEJO DE ERRORES\n",
        "\n",
        "### Timeouts largos\n",
        "\n",
        "Las llamadas a APIs de IA pueden tardar varios segundos (5-15s en prompts complejos). Debemos:\n",
        "\n",
        "1. **Configurar timeouts** para evitar requests colgados indefinidamente\n",
        "2. **Manejar TimeoutError** con mensajes claros al usuario\n",
        "3. **Usar asyncio.wait_for()** para limitar el tiempo de espera\n",
        "\n",
        "**Estrategia:**\n",
        "```python\n",
        "try:\n",
        "    response = await asyncio.wait_for(\n",
        "        model.generate_content_async(prompt),\n",
        "        timeout=10.0  # Máximo 10 segundos\n",
        "    )\n",
        "except asyncio.TimeoutError:\n",
        "    raise HTTPException(status_code=504, detail=\"La IA tardó demasiado en responder\")\n",
        "```\n",
        "\n",
        "**Códigos HTTP relevantes:**\n",
        "- `504 Gateway Timeout`: El servidor upstream (Gemini) no respondió a tiempo\n",
        "- `408 Request Timeout`: El cliente tardó demasiado en enviar la petición"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prueba 1: Timeout de 30 segundos (debería funcionar)\n",
            "  Estado: completado\n",
            "  Respuesta: ¡Claro que sí! FastAPI es un framework web de Pyth...\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "import asyncio\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from fastapi.testclient import TestClient\n",
        "\n",
        "# Configurar Gemini\n",
        "genai.configure(api_key=\"AIzaSyBS7oO2Bw7GbVOBz1xOh5XHSMaSSXr6xkg\")\n",
        "model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
        "\n",
        "app_con_timeout = FastAPI()\n",
        "\n",
        "class SolicitudIA(BaseModel):\n",
        "    prompt: str\n",
        "    timeout_segundos: int = 10\n",
        "\n",
        "@app_con_timeout.post(\"/generar-con-timeout\")\n",
        "async def generar_con_timeout(solicitud: SolicitudIA):\n",
        "    \"\"\"Endpoint con manejo de timeout\"\"\"\n",
        "    try:\n",
        "        # Configurar timeout con asyncio.wait_for\n",
        "        respuesta = await asyncio.wait_for(\n",
        "            model.generate_content_async(solicitud.prompt),\n",
        "            timeout=solicitud.timeout_segundos\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            \"respuesta\": respuesta.text[:100] + \"...\",\n",
        "            \"tokens_usados\": respuesta.usage_metadata.total_token_count,\n",
        "            \"estado\": \"completado\"\n",
        "        }\n",
        "        \n",
        "    except asyncio.TimeoutError:\n",
        "        raise HTTPException(\n",
        "            status_code=504,\n",
        "            detail=f\"La IA no respondió en {solicitud.timeout_segundos} segundos. Intenta con un prompt más corto.\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        raise HTTPException(\n",
        "            status_code=500,\n",
        "            detail=f\"Error inesperado: {str(e)}\"\n",
        "        )\n",
        "\n",
        "# Prueba con timeout generoso (debería funcionar)\n",
        "cliente = TestClient(app_con_timeout)\n",
        "\n",
        "print(\"Prueba 1: Timeout de 30 segundos (debería funcionar)\")\n",
        "respuesta = cliente.post(\"/generar-con-timeout\", json={\n",
        "    \"prompt\": \"Explica FastAPI\",\n",
        "    \"timeout_segundos\": 30\n",
        "})\n",
        "\n",
        "if respuesta.status_code == 200:\n",
        "    print(f\"  Estado: {respuesta.json()['estado']}\")\n",
        "    print(f\"  Respuesta: {respuesta.json()['respuesta'][:50]}...\")\n",
        "else:\n",
        "    print(f\"  Error {respuesta.status_code}: {respuesta.json()['detail']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Retry logic con backoff exponencial\n",
        "\n",
        "Si la API falla temporalmente (error 500, timeout, rate limit), podemos reintentar automáticamente con esperas crecientes.\n",
        "\n",
        "**¿Qué es backoff exponencial?**\n",
        "- Intento 1: inmediato\n",
        "- Intento 2: espera 1 segundo\n",
        "- Intento 3: espera 2 segundos  \n",
        "- Intento 4: espera 4 segundos\n",
        "- Intento 5: espera 8 segundos\n",
        "\n",
        "**¿Por qué es útil?**\n",
        "- Evita saturar la API con reintentos inmediatos\n",
        "- Da tiempo al servidor para recuperarse\n",
        "- Mejora la tasa de éxito en errores transitorios\n",
        "\n",
        "**Errores que vale la pena reintentar:**\n",
        "- 429 (Rate Limit)\n",
        "- 500, 502, 503 (Errores del servidor)\n",
        "- TimeoutError\n",
        "- Errores de red temporales\n",
        "\n",
        "**Errores que NO se deben reintentar:**\n",
        "- 400 (Bad Request) - El request está mal formado\n",
        "- 401 (Unauthorized) - API key inválida\n",
        "- 404 (Not Found) - Endpoint no existe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "import asyncio\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from fastapi.testclient import TestClient\n",
        "\n",
        "# Configurar Gemini\n",
        "genai.configure(api_key=\"AIzaSyBS7oO2Bw7GbVOBz1xOh5XHSMaSSXr6xkg\")\n",
        "model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
        "\n",
        "async def llamar_gemini_con_retry(\n",
        "    prompt: str, \n",
        "    max_intentos: int = 3, \n",
        "    timeout: float = 10.0\n",
        "):\n",
        "    \"\"\"\n",
        "    Llama a Gemini con retry logic y backoff exponencial\n",
        "    \n",
        "    Args:\n",
        "        prompt: Texto a enviar a Gemini\n",
        "        max_intentos: Número máximo de intentos (default: 3)\n",
        "        timeout: Timeout por intento en segundos\n",
        "    \n",
        "    Returns:\n",
        "        Respuesta de Gemini\n",
        "    \n",
        "    Raises:\n",
        "        Exception: Si todos los intentos fallan\n",
        "    \"\"\"\n",
        "    for intento in range(1, max_intentos + 1):\n",
        "        try:\n",
        "            print(f\"Intento {intento}/{max_intentos}...\")\n",
        "            \n",
        "            # Llamada con timeout\n",
        "            respuesta = await asyncio.wait_for(\n",
        "                model.generate_content_async(prompt),\n",
        "                timeout=timeout\n",
        "            )\n",
        "            \n",
        "            print(f\"  ✓ Éxito en intento {intento}\")\n",
        "            return respuesta\n",
        "            \n",
        "        except asyncio.TimeoutError:\n",
        "            print(f\"  ✗ Timeout en intento {intento}\")\n",
        "            \n",
        "            if intento == max_intentos:\n",
        "                raise Exception(f\"Falló después de {max_intentos} intentos (timeout)\")\n",
        "            \n",
        "            # Backoff exponencial: 2^(intento-1) segundos\n",
        "            espera = 2 ** (intento - 1)\n",
        "            print(f\"  ⏳ Esperando {espera}s antes del siguiente intento...\")\n",
        "            await asyncio.sleep(espera)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  ✗ Error en intento {intento}: {str(e)[:50]}\")\n",
        "            \n",
        "            if intento == max_intentos:\n",
        "                raise Exception(f\"Falló después de {max_intentos} intentos: {str(e)}\")\n",
        "            \n",
        "            # Backoff exponencial\n",
        "            espera = 2 ** (intento - 1)\n",
        "            print(f\"  ⏳ Esperando {espera}s antes del siguiente intento...\")\n",
        "            await asyncio.sleep(espera)\n",
        "\n",
        "# Aplicación con retry\n",
        "app_con_retry = FastAPI()\n",
        "\n",
        "class SolicitudIA(BaseModel):\n",
        "    prompt: str\n",
        "    max_intentos: int = 3\n",
        "\n",
        "@app_con_retry.post(\"/generar-con-retry\")\n",
        "async def generar_con_retry(solicitud: SolicitudIA):\n",
        "    \"\"\"Endpoint con retry logic integrado\"\"\"\n",
        "    try:\n",
        "        respuesta = await llamar_gemini_con_retry(\n",
        "            solicitud.prompt,\n",
        "            max_intentos=solicitud.max_intentos\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            \"respuesta\": respuesta.text[:100] + \"...\",\n",
        "            \"tokens_usados\": respuesta.usage_metadata.total_token_count\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        raise HTTPException(\n",
        "            status_code=500,\n",
        "            detail=f\"Error después de {solicitud.max_intentos} intentos: {str(e)}\"\n",
        "        )\n",
        "\n",
        "# Prueba\n",
        "cliente = TestClient(app_con_retry)\n",
        "\n",
        "print(\"=== Prueba de retry logic ===\\n\")\n",
        "respuesta = cliente.post(\"/generar-con-retry\", json={\n",
        "    \"prompt\": \"Explica FastAPI en una frase\",\n",
        "    \"max_intentos\": 3\n",
        "})\n",
        "\n",
        "if respuesta.status_code == 200:\n",
        "    print(f\"\\n✓ Respuesta final: {respuesta.json()['respuesta'][:80]}...\")\n",
        "    print(f\"\\n✓ ResTokens usados: {respuesta.json()['tokens_usados']}\")\n",
        "else:\n",
        "    print(f\"\\n✗ Error: {respuesta.json()['detail']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Logging de uso\n",
        "\n",
        "Es crítico registrar cada llamada a IA para:\n",
        "- **Monitoreo de costes:** Rastrear tokens consumidos y calcular gastos\n",
        "- **Debugging:** Identificar prompts problemáticos o lentos\n",
        "- **Auditoría:** Registrar quién usa la API y cuándo\n",
        "- **Optimización:** Detectar patrones de uso para mejorar prompts\n",
        "\n",
        "**Información a loggear:**\n",
        "- Timestamp\n",
        "- User ID\n",
        "- Prompt (primeros 50 caracteres por privacidad)\n",
        "- Tokens de entrada y salida\n",
        "- Tiempo de respuesta\n",
        "- Coste estimado\n",
        "- Éxito/error\n",
        "\n",
        "**Niveles de logging:**\n",
        "- `INFO`: Llamadas exitosas\n",
        "- `WARNING`: Rate limits, timeouts recuperados\n",
        "- `ERROR`: Fallos permanentes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "import asyncio\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from fastapi import FastAPI, HTTPException, Header\n",
        "from pydantic import BaseModel\n",
        "from fastapi.testclient import TestClient\n",
        "from typing import Annotated\n",
        "\n",
        "# Configurar logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(\"gemini_api\")\n",
        "\n",
        "# Configurar Gemini\n",
        "genai.configure(api_key=\"AIzaSyBS7oO2Bw7GbVOBz1xOh5XHSMaSSXr6xkg\")\n",
        "model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
        "\n",
        "# Precios de Gemini 2.5 Flash (después de límite gratuito)\n",
        "PRECIO_INPUT = 0.075 / 1_000_000  # Por token\n",
        "PRECIO_OUTPUT = 0.30 / 1_000_000\n",
        "\n",
        "async def llamar_gemini_con_logging(prompt: str, user_id: str):\n",
        "    \"\"\"Llamada a Gemini con logging completo\"\"\"\n",
        "    inicio = datetime.now()\n",
        "    \n",
        "    try:\n",
        "        # Log de inicio\n",
        "        logger.info(\n",
        "            f\"[INICIO] user_id={user_id} | \"\n",
        "            f\"prompt='{prompt[:50]}{'...' if len(prompt) > 50 else ''}'\"\n",
        "        )\n",
        "        \n",
        "        # Llamada a Gemini\n",
        "        respuesta = model.generate_content(prompt)\n",
        "        \n",
        "        # Calcular métricas\n",
        "        tiempo_ms = (datetime.now() - inicio).total_seconds() * 1000\n",
        "        tokens_input = respuesta.usage_metadata.prompt_token_count\n",
        "        tokens_output = respuesta.usage_metadata.candidates_token_count\n",
        "        tokens_total = respuesta.usage_metadata.total_token_count\n",
        "        \n",
        "        # Calcular coste\n",
        "        coste = (tokens_input * PRECIO_INPUT) + (tokens_output * PRECIO_OUTPUT)\n",
        "        \n",
        "        # Log de éxito\n",
        "        logger.info(\n",
        "            f\"[ÉXITO] user_id={user_id} | \"\n",
        "            f\"tokens={tokens_total} (in:{tokens_input} out:{tokens_output}) | \"\n",
        "            f\"tiempo={tiempo_ms:.0f}ms | \"\n",
        "            f\"coste=${coste:.6f}\"\n",
        "        )\n",
        "        \n",
        "        return respuesta, coste\n",
        "        \n",
        "    except Exception as e:\n",
        "        # Log de error\n",
        "        tiempo_ms = (datetime.now() - inicio).total_seconds() * 1000\n",
        "        logger.error(\n",
        "            f\"[ERROR] user_id={user_id} | \"\n",
        "            f\"tiempo={tiempo_ms:.0f}ms | \"\n",
        "            f\"error={str(e)[:100]}\"\n",
        "        )\n",
        "        raise\n",
        "\n",
        "# Aplicación con logging\n",
        "app_con_logging = FastAPI()\n",
        "\n",
        "class SolicitudIA(BaseModel):\n",
        "    prompt: str\n",
        "\n",
        "@app_con_logging.post(\"/generar-con-logging\")\n",
        "async def generar_con_logging(\n",
        "    solicitud: SolicitudIA,\n",
        "    user_id: Annotated[str, Header()] = \"anonimo\"\n",
        "):\n",
        "    \"\"\"Endpoint con logging completo de uso\"\"\"\n",
        "    try:\n",
        "        respuesta, coste = await llamar_gemini_con_logging(\n",
        "            solicitud.prompt,\n",
        "            user_id\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            \"respuesta\": respuesta.text[:100] + \"...\",\n",
        "            \"tokens_usados\": respuesta.usage_metadata.total_token_count,\n",
        "            \"coste_usd\": round(coste, 6)\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "# Prueba\n",
        "cliente = TestClient(app_con_logging)\n",
        "\n",
        "print(\"=== Prueba de logging (revisa los logs arriba) ===\\n\")\n",
        "respuesta = cliente.post(\n",
        "    \"/generar-con-logging\",\n",
        "    json={\"prompt\": \"Explica FastAPI en 2 frases\"},\n",
        "    headers={\"user-id\": \"usuario_123\"}\n",
        ")\n",
        "\n",
        "if respuesta.status_code == 200:\n",
        "    datos = respuesta.json()\n",
        "    print(f\"Tokens: {datos['tokens_usados']}\")\n",
        "    print(f\"Coste: ${datos['coste_usd']}\")\n",
        "else:\n",
        "    print(f\"Error: {respuesta.json()['detail']}\")\n",
        "\n",
        "run_api(app_con_logging)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Endpoint completo con mejores prácticas\n",
        "\n",
        "Integrando todo lo aprendido: rate limiting, timeout, retry, logging y manejo de errores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "import asyncio\n",
        "import logging\n",
        "from datetime import datetime, timedelta\n",
        "from collections import defaultdict\n",
        "from fastapi import FastAPI, HTTPException, Header\n",
        "from pydantic import BaseModel\n",
        "from fastapi.testclient import TestClient\n",
        "from typing import Annotated\n",
        "\n",
        "# Configuración\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(\"api_completa\")\n",
        "\n",
        "genai.configure(api_key=\"AIzaSyBS7oO2Bw7GbVOBz1xOh5XHSMaSSXr6xkg\")\n",
        "model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
        "\n",
        "# Rate limiting\n",
        "rate_limit_store = defaultdict(list)\n",
        "\n",
        "def verificar_rate_limit(user_id: str, max_requests: int = 5) -> bool:\n",
        "    ahora = datetime.now()\n",
        "    inicio_ventana = ahora - timedelta(minutes=1)\n",
        "    requests_recientes = [t for t in rate_limit_store[user_id] if t > inicio_ventana]\n",
        "    rate_limit_store[user_id] = requests_recientes\n",
        "    \n",
        "    if len(requests_recientes) >= max_requests:\n",
        "        return False\n",
        "    rate_limit_store[user_id].append(ahora)\n",
        "    return True\n",
        "\n",
        "# Aplicación completa\n",
        "app_produccion = FastAPI()\n",
        "\n",
        "class SolicitudIA(BaseModel):\n",
        "    prompt: str\n",
        "    max_tokens: int = 150\n",
        "\n",
        "@app_produccion.post(\"/ia/generar\")\n",
        "async def endpoint_produccion(\n",
        "    solicitud: SolicitudIA,\n",
        "    user_id: Annotated[str, Header()] = \"anonimo\"\n",
        "):\n",
        "    \"\"\"Endpoint de producción con todas las mejores prácticas integradas\"\"\"\n",
        "    inicio = datetime.now()\n",
        "    \n",
        "    try:\n",
        "        # 1. Validación de input\n",
        "        if solicitud.max_tokens > 500:\n",
        "            logger.warning(f\"user_id={user_id} intentó usar max_tokens={solicitud.max_tokens}\")\n",
        "            raise HTTPException(\n",
        "                status_code=400,\n",
        "                detail=\"max_tokens no puede exceder 500\"\n",
        "            )\n",
        "        \n",
        "        # 2. Rate limiting\n",
        "        if not verificar_rate_limit(user_id, max_requests=5):\n",
        "            logger.warning(f\"Rate limit excedido para user_id={user_id}\")\n",
        "            raise HTTPException(\n",
        "                status_code=429,\n",
        "                detail=\"Límite de 5 requests por minuto excedido\"\n",
        "            )\n",
        "        \n",
        "        # 3. Log de inicio\n",
        "        logger.info(\n",
        "            f\"[INICIO] user_id={user_id} | prompt='{solicitud.prompt[:30]}...'\"\n",
        "        )\n",
        "        \n",
        "        # 4. Llamada con timeout\n",
        "        config = genai.types.GenerationConfig(max_output_tokens=solicitud.max_tokens)\n",
        "        \n",
        "        respuesta = await asyncio.wait_for(\n",
        "            model.generate_content_async(solicitud.prompt, generation_config=config),\n",
        "            timeout=15.0\n",
        "        )\n",
        "        \n",
        "        # 5. Métricas\n",
        "        duracion_ms = (datetime.now() - inicio).total_seconds() * 1000\n",
        "        tokens = respuesta.usage_metadata.total_token_count\n",
        "        \n",
        "        # 6. Log de éxito\n",
        "        logger.info(\n",
        "            f\"[ÉXITO] user_id={user_id} | tokens={tokens} | tiempo={duracion_ms:.0f}ms\"\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            \"respuesta\": respuesta.text[:200] + \"...\" if len(respuesta.text) > 200 else respuesta.text,\n",
        "            \"tokens_usados\": tokens,\n",
        "            \"duracion_ms\": int(duracion_ms)\n",
        "        }\n",
        "        \n",
        "    except asyncio.TimeoutError:\n",
        "        logger.error(f\"[TIMEOUT] user_id={user_id} después de 15s\")\n",
        "        raise HTTPException(\n",
        "            status_code=504,\n",
        "            detail=\"La IA tardó demasiado en responder\"\n",
        "        )\n",
        "    except HTTPException:\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        logger.error(f\"[ERROR] user_id={user_id} | error={str(e)[:100]}\")\n",
        "        raise HTTPException(\n",
        "            status_code=500,\n",
        "            detail=\"Error interno del servidor\"\n",
        "        )\n",
        "\n",
        "# Prueba\n",
        "cliente = TestClient(app_produccion)\n",
        "\n",
        "print(\"=== Endpoint de producción ===\\n\")\n",
        "respuesta = cliente.post(\n",
        "    \"/ia/generar\",\n",
        "    json={\"prompt\": \"Explica FastAPI\", \"max_tokens\": 100},\n",
        "    headers={\"user-id\": \"usuario_demo\"}\n",
        ")\n",
        "\n",
        "if respuesta.status_code == 200:\n",
        "    datos = respuesta.json()\n",
        "    print(f\"✓ Tokens: {datos['tokens_usados']}\")\n",
        "    print(f\"✓ Duración: {datos['duracion_ms']}ms\")\n",
        "    print(f\"✓ Respuesta: {datos['respuesta'][:80]}...\")\n",
        "else:\n",
        "    print(f\"✗ Error {respuesta.status_code}: {respuesta.json()['detail']}\")\n",
        "\n",
        "print(\"\\n✓ Incluye: validación, rate limiting, timeout, logging y manejo de errores\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MICRO-RETO 5: Endpoint con logging completo\n",
        "\n",
        "Añade logs INFO y WARNING al sistema de rate limiting para monitorear su uso.\n",
        "\n",
        "**Tareas:**\n",
        "1. En `verificar_rate_limit()`: Añadir log INFO mostrando requests actuales vs límite\n",
        "2. Cuando se exceda el límite: Añadir log WARNING antes de retornar False\n",
        "3. Test: Hacer 6 requests (límite es 5) y verificar que aparecen los logs\n",
        "\n",
        "**Logs esperados:**\n",
        "```\n",
        "INFO - [RATE LIMIT] user=test_user requests=3/5\n",
        "INFO - [RATE LIMIT] user=test_user requests=4/5\n",
        "WARNING - [RATE LIMIT EXCEEDED] user=test_user\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "from collections import defaultdict\n",
        "from datetime import datetime, timedelta\n",
        "from fastapi import FastAPI, HTTPException, Header\n",
        "from pydantic import BaseModel\n",
        "from fastapi.testclient import TestClient\n",
        "from typing import Annotated\n",
        "\n",
        "# Configurar logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(\"rate_limit\")\n",
        "\n",
        "# Store de rate limiting\n",
        "rate_limit_store = defaultdict(list)\n",
        "\n",
        "def verificar_rate_limit(user_id: str, max_requests: int = 5) -> bool:\n",
        "    \"\"\"Verifica rate limit con logging completo\"\"\"\n",
        "    ahora = datetime.now()\n",
        "    inicio_ventana = ahora - timedelta(minutes=1)\n",
        "    \n",
        "    requests_recientes = [\n",
        "        t for t in rate_limit_store[user_id] if t > inicio_ventana\n",
        "    ]\n",
        "    rate_limit_store[user_id] = requests_recientes\n",
        "    \n",
        "    # TODO: Añade log INFO aquí\n",
        "    # logger.info(f\"[RATE LIMIT] user={user_id} requests={len(requests_recientes)}/{max_requests}\")\n",
        "    \n",
        "    if len(requests_recientes) >= max_requests:\n",
        "        # TODO: Añade log WARNING aquí ANTES de retornar False\n",
        "        # logger.warning(f\"[RATE LIMIT EXCEEDED] user={user_id}\")\n",
        "        return False\n",
        "    \n",
        "    rate_limit_store[user_id].append(ahora)\n",
        "    return True\n",
        "\n",
        "# Aplicación\n",
        "app_rate_limit = FastAPI()\n",
        "\n",
        "class Solicitud(BaseModel):\n",
        "    mensaje: str\n",
        "\n",
        "@app_rate_limit.post(\"/mensaje\")\n",
        "async def enviar_mensaje(\n",
        "    solicitud: Solicitud,\n",
        "    user_id: Annotated[str, Header()] = \"anonimo\"\n",
        "):\n",
        "    \"\"\"Endpoint con rate limiting\"\"\"\n",
        "    if not verificar_rate_limit(user_id, max_requests=5):\n",
        "        raise HTTPException(\n",
        "            status_code=429,\n",
        "            detail=\"Rate limit excedido\"\n",
        "        )\n",
        "    \n",
        "    return {\"respuesta\": f\"Mensaje recibido: {solicitud.mensaje}\"}\n",
        "\n",
        "# TODO: Test - Descomenta y ejecuta\n",
        "cliente = TestClient(app_rate_limit)\n",
        "print(\"=== Test de logging (límite: 5 requests) ===\\n\")\n",
        "\n",
        "for i in range(1, 7):  # 6 requests, límite es 5\n",
        "    respuesta = cliente.post(\n",
        "        \"/mensaje\",\n",
        "        json={\"mensaje\": f\"Test {i}\"},\n",
        "        headers={\"user-id\": \"test_user\"}\n",
        "    )\n",
        "    \n",
        "    if respuesta.status_code == 200:\n",
        "        print(f\"Request {i}: OK\")\n",
        "    else:\n",
        "        print(f\"Request {i}: BLOQUEADO\")\n",
        "\n",
        "print(\"\\n¡Revisa los logs arriba! Deberías ver logs INFO y WARNING\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
